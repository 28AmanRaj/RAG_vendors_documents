import streamlit as st
from llama_parse import LlamaParse, ResultType
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from typing import List
import os
import pickle
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
import asyncio
import nest_asyncio
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
nest_asyncio.apply()



def store_in_faiss_db(langchain_documents):
    """
    Stores LangChain documents in a FAISS vector database.
    Handles saving/loading the FAISS index and document metadata separately.
    """
    faiss_index_path = "faiss_index"
    metadata_path = "faiss_metadata.pkl"
    embedding_model = OpenAIEmbeddings()

    if os.path.exists(faiss_index_path) and os.path.exists(metadata_path):
        st.info(f"‚úÖ Loading existing FAISS DB from `{faiss_index_path}`")
        
        # Load FAISS index
        faiss_index = faiss.read_index(faiss_index_path)
        
        # Load metadata safely
        with open(metadata_path, "rb") as f:
            saved_data = pickle.load(f)

        # Ensure docstore is of type InMemoryDocstore
        if isinstance(saved_data, InMemoryDocstore):
            docstore = saved_data
            index_to_docstore_id = {i: k for i, k in enumerate(docstore._dict.keys())}  # Rebuild ID mapping
        elif isinstance(saved_data, dict):
            docstore = saved_data.get("docstore", InMemoryDocstore({}))
            index_to_docstore_id = saved_data.get("index_to_docstore_id", {})
        else:
            st.error("‚ùå Invalid FAISS metadata format.")
            return None

        # Reconstruct FAISS VectorStore
        vector_db = FAISS(
            embedding_model,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id
        )
    
    else:
        st.warning("‚ö†Ô∏è FAISS DB not found! Creating new embeddings...")
        
        # Create FAISS VectorStore
        vector_db = FAISS.from_documents(langchain_documents, embedding_model)
        
        # Save FAISS index
        faiss.write_index(vector_db.index, faiss_index_path)
        
        # Save document metadata separately
        with open(metadata_path, "wb") as f:
            pickle.dump(
                {"docstore": vector_db.docstore, "index_to_docstore_id": vector_db.index_to_docstore_id},
                f
            )

        st.success(f"‚úÖ FAISS DB created and saved at `{faiss_index_path}`")
        

    return vector_db




system_message = """"You are an expert in analyzing vendor documents. Your task is to answer user questions using only the content given to you. You will be provided with question and context.
   **Instructions for Answering User Queries:**

    1. **Review the Question:**  
      - Analyze the user's question and determine the required information.

    2. **Check for Answer in the Provided Context:**  
      - Search for all relevant chunks in the provided context that directly answer the question.
      - Extract the exact PDF name and page number where the relevant information is found.

    3. **Citation & Control Numbers Extraction:**
      - If the context contains relevant information:
        - Extract **all matching PDF names and their page numbers** where the answer is found.
        - Extract **all control numbers** that fully satisfy the question.
      
    4. **If No Sufficient Information is Found:**
      - Respond with **‚ÄúNeeds Additional Information‚Äù** instead of guessing.

    **Output Format:**
        Provide a concise answer based on the context.
        When providing your final answer, strictly adhere to the answer Format:    <Answer>: ...  <Citation(s)>: ...  <Controls or Appendix>: ...  <Result>: Pass / Fail / Needs Additional Information.


        Answer: [Your concise response]
        Citation(s): [PDF name, page number, section, or specific text snippet]
        Controls or Appendix: [Relevant control numbers]
        Result: [Pass / Fail / Needs Additional Information]

    **Note:**
      citation number can be anything like (CC 1.2, CC5.3, PI 1.2, A2.1, etc) combination of alpha-numeric.
      Before adding citation, make sure that the information is correct.
      There can be more than one relevant pdf, add all relevant pdf name and respective page number in citation section seperated by ",".
      DO NOT halucinate with wrong infromation.
      Only use context to answer question, do not use your own knowledge
      If multiple documents contain relevant or overlapping information, **prioritize the document with ‚ÄúSOC_2‚Äù in the file name** while still listing others.

    """

def answer_query(query, retriever,system_prompt):
    # Retrieve relevant document chunks
    hybrid_chunks = retriever.invoke(query)
    context = ""

    for chunk in hybrid_chunks:
        file_name = chunk.metadata.get("file_name", "Unknown")  # Extract file name
        context += f"üìÑ **File:** {file_name}\nüîπ **Content:** {chunk.page_content}\n\n."

    # Format prompt for the LLM
    system_message = system_prompt
    human_message = f"üîé **Query:** {query}\n\nüìú **Context:** {context} "

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": human_message}
    ]

    # Call LLM to generate answer
    llm = ChatOpenAI(model="gpt-4o",temperature=0)
    response = llm.invoke(messages)

    return response

def convert_to_langchain_documents(parsed_files):
    langchain_docs = []
    for file_path in parsed_files:
        with open(file_path, "rb") as file:  
            documents = pickle.load(file)  

        
        for doc in documents:
            langchain_doc = Document(page_content=doc.text, metadata=doc.metadata)
            langchain_docs.append(langchain_doc)

    return langchain_docs

def create_hybrid_retriever(vector_db, langchain_documents):
    vectorstore_retriever = vector_db.as_retriever(search_kwargs={"k": 8})
    keyword_retriever = BM25Retriever.from_documents(langchain_documents, k=4)

    ensemble_retriever = EnsembleRetriever(
        retrievers=[vectorstore_retriever, keyword_retriever],
        weights=[0.6, 0.4]  
    )

    return ensemble_retriever

# def store_in_chroma_db(langchain_documents):
#     chroma_db_path = "chroma_db"  
#     embedding_model = OpenAIEmbeddings()

#     if os.path.exists(chroma_db_path):
#         vector_db = Chroma(persist_directory=chroma_db_path, embedding_function=embedding_model)
#         st.info(f"‚úÖ Loaded existing ChromaDB from `{chroma_db_path}`")
#     else:
#         st.warning("‚ö†Ô∏è ChromaDB not found! Creating new embeddings...")
#         vector_db = Chroma.from_documents(langchain_documents, embedding_model, persist_directory=chroma_db_path)
#         st.success(f"‚úÖ ChromaDB created and saved at `{chroma_db_path}`")

#     # Verify stored document count
#     num_vectors = vector_db._collection.count()
#     st.write(f"üî¢ **Total documents indexed in ChromaDB:** {num_vectors}")

#     return vector_db
st.set_page_config(page_title="RAG Vendor Documents", layout="wide")
st.title("üìÑ RAG Vendor Document Processor")
st.sidebar.header("‚öôÔ∏è Configuration")

# Ensure API keys exist in session state
st.session_state.setdefault("llama_api_key", "")
st.session_state.setdefault("openai_api_key", "")

# Input for API keys
llama_api_key = st.sidebar.text_input("Llama Cloud API Key", type="password")
openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")

# Update session state only if a new key is provided
if llama_api_key:
    st.session_state.llama_api_key = llama_api_key
    os.environ["LLAMA_CLOUD_API_KEY"] = llama_api_key  # Set environment variable

if openai_api_key:
    st.session_state.openai_api_key = openai_api_key
    os.environ["OPENAI_API_KEY"] = openai_api_key  # Set environment variable

# Stop execution if keys are missing
if not st.session_state.llama_api_key or not st.session_state.openai_api_key:
    st.error("‚ùå Please enter both Llama Cloud API Key and OpenAI API Key before uploading documents.")
    st.stop()




uploaded_files = st.sidebar.file_uploader("Upload vendor documents", accept_multiple_files=True, type="pdf")




def load_and_process_documents(file_paths: List[str], output_dir: str):
    """
    Parses documents using LlamaParse and saves them as Pickle files.
    Uses caching to avoid re-parsing if already processed.
    """
    st.write("üìå **Parsing documents...**")

    system_prompt_append = """
    If a row in table does not contain a value in the No. column, use the last available No. from the previous rows in the same table.
    If a table starts without an explicit No. in the first row, use the last No. from the previous table.
    If table's first row does not contain a value in the No. column, then use the last value seen to populate it.
    """

    parser = LlamaParse(result_type=ResultType.MD, system_prompt_append=system_prompt_append, parsing_mode="parse_document_with_llm")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    parsed_files = []
    progress_bar = st.progress(0)  # Streamlit progress bar

    for idx, file_path in enumerate(file_paths):
        file_name = os.path.basename(file_path)
        output_file = os.path.join(output_dir, f"parsed_{file_name}.pkl")

        if os.path.exists(output_file):
            st.info(f"‚úÖ Loading cached document: **{file_name}**")
            with open(output_file, 'rb') as file:
                documents = pickle.load(file)
        else:
            st.warning(f"‚ö†Ô∏è Parsing document: **{file_name}** ...")
            try:
                documents = parser.load_data(file_path)  # ‚úÖ No async handling needed

                # Add metadata
                for doc in documents:
                    if not hasattr(doc, "metadata"):
                        doc.metadata = {}
                    doc.metadata["file_name"] = file_name

                # Save parsed document
                with open(output_file, 'wb') as file:
                    pickle.dump(documents, file)

            except Exception as e:
                st.error(f"‚ùå Error while parsing '{file_name}': {e}")
                continue

        parsed_files.append(output_file)
        progress_bar.progress((idx + 1) / len(file_paths))
    st.success("‚úÖ All documents processed successfully!")
    return parsed_files

if uploaded_files:
    output_dir = "processed_docs"
    os.makedirs(output_dir, exist_ok=True)

    # Save uploaded files to local directory
    file_paths = []
    for uploaded_file in uploaded_files:
        file_path = os.path.join(output_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        file_paths.append(file_path)

    # üõ† Call the function to process documents
    parsed_files = load_and_process_documents(file_paths, output_dir)

    langchain_documents = convert_to_langchain_documents(parsed_files)
    if langchain_documents:

        st.success(f"‚úÖ Converted {len(langchain_documents)} documents into LangChain format!")
        st.write(f"üìù **Total Parsed Files:** {len(parsed_files)}")
        st.write(f"üìÑ **Total LangChain Documents:** {len(langchain_documents)}")

    # Validate documents
    all_docs_valid = all(isinstance(doc, Document) for doc in langchain_documents)
    st.write(f"‚úÖ **All files converted correctly:** {all_docs_valid}")


    # Store documents in ChromaDB
    # vector_db = store_in_chroma_db(langchain_documents)
    vector_db = store_in_faiss_db(langchain_documents)



    retriever = create_hybrid_retriever(vector_db, langchain_documents)


    st.write("## üîé Ask Multiple Questions")
    user_queries = st.text_area("Enter multiple queries (one per line):")

    if user_queries:
        queries = user_queries.strip().split("\n")  # Split input into separate queries

        for i, query in enumerate(queries, start=1):
            st.write(f"### üîπ Query {i}: {query}")  
            response = answer_query(query, retriever, system_message)  # Process each query
            if hasattr(response, "content"):  
                clean_response = response.content  # Extract only the answer text
            else:
                clean_response = str(response)  # Fallback in case response is a plain string

            st.success(clean_response)